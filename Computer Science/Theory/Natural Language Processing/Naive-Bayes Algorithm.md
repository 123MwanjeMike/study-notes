> All information gathered from Udacity's Natual Language Processing Nanodegree

# Naive-Bayes for NLP

## Bayes Theorem

Bayes Theorem constructs a probability of the occurrence of an event by aggregating known probabilities on smaller events that build up to the one that we want to understand.

It compounds of two sets of probabilities called: Inferior and Posterior, named by the time of knowledge of the events' probabilities.

It stands on this premises to move from a Known probability to an Inferred probability

#### Known probabilities
$$
P(A)
$$
$$
P(R|A)
$$

#### Inferred probability
$$
P(A|R)
$$

### Example

#### Priors (Probability of seeing someone)
Alex goes to office 3 days a week (4-day week)
$$P(Alex) = 0.75$$
Brenda goes to office 1 day a week (4-day week)
$$P(Brenda)=0.25$$

> Written with [StackEdit](https://stackedit.io/).
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTkzNzEzMTcxMiw5NDk2NzMwNDJdfQ==
-->